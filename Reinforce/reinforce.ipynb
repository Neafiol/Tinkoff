{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "Пусть $X$ это какая-то случайная величина с известным распределением $p_\\theta(X)$, где $\\theta$ — это параметры модели. Определим функцию награды $J(\\theta)$ как\n",
    "\n",
    "$$ J(\\theta) = E[f(X)] = \\int_x f(x) p(x) \\; dx $$\n",
    "\n",
    "для произвольной функции $f$, не зависящей от $\\theta$.\n",
    "\n",
    "*Policy gradient* (дословно, градиент стратегии) — это, интуитивно, то направление, куда нужно двигать параметры модели, чтобы функция награды увеличивалась. Алгоритм REINFORCE (почему его всегда пишут капсом автор не знает) на каждом шаге просто определяет policy gradient $\\nabla_\\theta J(\\theta)$ и изменяет параметры в его сторону с каким-то learning rate-ом.\n",
    "\n",
    "**Зачем это надо**, если есть обычный градиентный спуск? Через policy gradient и reinforcement learning вообще можно оптимизировать более общий класс функций — хоть дискретные (например, можно BLEU для перевода напрямую максимизировать) и даже невычислимые (какие-нибудь субъективные оценки асессоров).\n",
    "\n",
    "В частности, в таком ключе можно описать игры (для простоты, однопользовательские): есть какие-то награды за совершение каких-то действий (для шахмат: +1 за победу, 0 за ничью, -1 за поражение; для тетриса: +0.1 за «выживание» ещё одну секунду, 1 за удаление слоя, 0 за проигрыш) и нам нужно подобрать такие параметры модели, чтобы максимизировать ожидаемую сумму наград за действия, которые мы совершили, то есть в точности $J(\\theta)$.\n",
    "\n",
    "Теперь немного математики: как нам найти этот $\\nabla_\\theta J(\\theta)$? Оказывается, мы можем выразить его ожидание, а тогда приблизительный градиент можно будет находить сэмплированием и усреднением градиентов — так же, как мы обычно обучаем нейросети на батчах.\n",
    "\n",
    "$$ J(\\theta) = E[f(X)] = \\int_x f(x) p(x) \\; dx = \\int_x f(x) p(x) \\nabla_\\theta \\log p(x) \\; dx = E[f(x) \\nabla_\\theta \\log p(x)] $$\n",
    "\n",
    "Переход между 2 и 3 верен, потому что $\\nabla \\log p(x) = \\frac{\\nabla p(x)}{p(x)}$ (просто подставьте и $p(x)$ сократится). Это называют log-derivative trick.\n",
    "\n",
    "Мы научились получить приблизительный градиент через сэмплирование. Давайте теперь что-нибудь обучим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CartPole-v0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про задачу можно почитать тут: https://gym.openai.com/envs/CartPole-v0/. Tl;dr: есть вертикально стоящая палка на подвижной платформе; нужно двигать платформу так, чтобы палка не упала.\n",
    "\n",
    "<img width='500px' src='https://preview.redd.it/sqjzj2cgnpt21.gif?overlay-align=bottom,left&overlay-pad=8,16&crop=1200:628.272251309,smart&overlay-height=0.10&overlay=%2Fv9vyirk6hl221.png%3Fs%3Db466421949eb723078743745ce6421609d7a9c66&width=1200&height=628.272251309&s=ba84ac5a9c14946456808c15f2754cb7369b8de9'>\n",
    "\n",
    "OpenAI в 2016-м году выпустили `gym` — библиотечку для абстрагирования RL-ных сред от алгоритма. Есть абстрактная *среда* (`env`), в ней есть какие-то *состояния* (`state`), из каждого состояния есть какой-то фиксированный набо *действий* (`action`), ведущих (возможно, с какими-то вероятностями) в другие состояния, и за разные действия в разных состояниях дается какая-то *награда* (`reward`). Как конкретно устроена игра, нам думать не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bfa96e6dd8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сеть\n",
    "\n",
    "Для REINFORCE вам нужна модель, которая берёт на вход состояние (каким-то образом закодированное) и возвращает вероятностное распределение действий в нём.\n",
    "\n",
    "Старайтесь не перемудрить — в общем случае сети для RL могут быть [довольно сложными](https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf), но CartPole не стоит того, чтобы писать глубокие архитектуры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = nn.Sequential(\n",
    "    nn.Linear(4,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,2),\n",
    "    # ...\n",
    "    # какие-нибудь релушки и линеары\n",
    "    # ...\n",
    "    nn.LogSoftmax(dim=1)  # важно, что на выходе должны быть не вероятности, а логиты\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym работает с numpy, а не напрямую с фреймворками. Для удобства, напишите функцию-обёртку, которая принимает состояния (`numpy array` размера `[batch, state_shape]`) и возвращает вероятности (размера `[batch, n_actions]]`, должны суммироваться в единицу)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(states):\n",
    "    t=torch.tensor(states,dtype=torch.float32)\n",
    "    pred=agent(t).detach().exp().numpy()\n",
    "    \n",
    "    # сконвертируйте состояния в тензор\n",
    "    # вычислите логиты\n",
    "    # вызовите софтмакс, чтобы получить веряотности\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_proba(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis = 1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probas.shape\n",
    "# test_probas.reshape(5,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовый прогон\n",
    "\n",
    "Хоть наша модель не обучена, её уже можно использовать, чтобы играть в произвольной среде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(2, 1, p=[0.1, 0.9])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    Играет одну сессию REINFORCE-агентом и возвращает последовательность состояний,\n",
    "    действий и наград, которые потом будут использоваться при обучении.\n",
    "    \"\"\"\n",
    "    \n",
    "    # тут будем хранить сессию\n",
    "    states, actions, rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # вероятности следующих действий, aka p(a|s)\n",
    "        action_probas = predict_proba(np.array([s]))[0] \n",
    "        \n",
    "        # сэмплируйте оттуда действие (посказка: np.random.choice)\n",
    "        a = np.random.choice(2, 1, p=action_probas)[0]\n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# протестируйте\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    s=0\n",
    "    g=gamma\n",
    "    res=[]\n",
    "    \n",
    "    for r in rewards[::-1]:\n",
    "        s=s*g+r\n",
    "        res.append(s)\n",
    "        \n",
    "    \"\"\"\n",
    "    Принимает массив ревардов и возвращает discounted массив по следующей формуле:\n",
    "    \n",
    "        G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    Тут нет ничего сложного -- итерируйтесь от последнего до первого\n",
    "    и насчитывайте G_t = r_t + gamma*G_{t+1} рекуррентно.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ...\n",
    "    return res[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вроде норм\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "                   [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "                   [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "                   [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"Вроде норм\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "Вспомним, что нам нужно оптимизировать\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Используя REINFORCE, нам в алгоритме по сути нужно максимизировать немного другую функцию:\n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "Когда мы будем вычислять её градиент, мы получим в точности policy gradient из REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, n_dims=None):\n",
    "    \"\"\" Конвертирует целочисленный вектор в one-hot матрицу. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут определите оптимизатор для модели\n",
    "# например, Adam с дефолтными параметрами\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr = 0.001)\n",
    " \n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma = 0.99):\n",
    "    \n",
    "\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    \n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "    \n",
    "    logprobas = agent(states)\n",
    "    probas = logprobas.exp()\n",
    "\n",
    "    \n",
    "    # выберем и просуммируем лог-вероятности только для выбранных действий\n",
    "    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim=1)\n",
    "    \n",
    "    J_hat = logprobas_for_actions.dot(cumulative_returns)\n",
    "    \n",
    "    # опционально: энтропийная регуляризация\n",
    "    entropy_reg = - ( torch.log(probas).view(-1) ).dot( probas.view(-1) )\n",
    "    \n",
    "    loss = - J_hat - 0.001 * entropy_reg\n",
    "    \n",
    "    \n",
    "    # шагните в сторону градиента\n",
    "    # ....\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # верните ревард сессии, чтобы потом их печатать\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Само обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:42.160\n",
      "mean reward:142.580\n",
      "mean reward:506.880\n",
      "Победа!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [train_on_session(*generate_session()) for _ in range(100)]\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 500:\n",
    "        print (\"Победа!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Видосик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
