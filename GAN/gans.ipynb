{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gans.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neafiol/Tinkoff/blob/master/GAN/gans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8coLQ3KOu4cA"
      },
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "Применение adversarial loss (более общей идеи, лежащей в основе GANов) позволило решить задачи, которые казались невозможными:\n",
        "\n",
        "* [Машинный перевод без параллельных данных](https://arxiv.org/pdf/1710.11041.pdf)\n",
        "* [Циклоганы: перевод изображений в другой домен](https://arxiv.org/abs/1703.10593)\n",
        "* Колоризация и [Super Resolution](https://arxiv.org/abs/1807.02758)\n",
        "* [Генерация и морфинг произвольных данных](https://arxiv.org/pdf/1809.11096.pdf) ([тут](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb#scrollTo=HuCO9tv3IKT2) можно поиграться с генерацией бургеров)\n",
        "* Применения в борьбе с adversarial атаками\n",
        "\n",
        "Вот постоянно пополняющийся список приложений GANов: https://github.com/nashory/gans-awesome-applications\n",
        "\n",
        "Сама [статья](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) Яна Гудфеллоу про GANы вышла в конце 2014 года и была процитирована 7687 раз за 4 года.\n",
        "\n",
        "\n",
        "<img width='500px' src='https://cdn-images-1.medium.com/max/800/1*eWURQXT41pwHvDg1xDiEmw.png'>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NZdtbv7Qu4cE"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь немного формальных определений:\n",
        "\n",
        "* Пусть $z$ — это вектор из латентного пространства, насэмпленный из нормального распределения.\n",
        "* $G(z)$ обозначает функцию генератора, которая отображает латентный вектор в пространство данных. Цель $G$ — оценить истинное распределение данных $p_d$, чтобы сэмплировать данные из оцененного распределения $p_g$.\n",
        "* $D(G(z))$ это вероятность (число от 0 до 1), что выход генератора $G$ является реальным изображением.\n",
        "\n",
        "$D$ и $G$ играют в минимаксную игру, в которой $D$ старается максимизировать вероятность, что он правильно классифицирует реальные и сгенерированные сэмплы, а $G$ старается минимизировать эту вероятность:\n",
        "\n",
        "$$\\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(x)))\\big]$$\n",
        "\n",
        "[Выясняется](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), что решение в этой минимаксной игре достигается при $p_g = p_d$ (и дискриминатор в этом случае может угадывать случайно). В реальности модели не всегда могут сойтись к этой точке.\n",
        "\n",
        "[DCGAN](https://arxiv.org/pdf/1511.06434.pdf) (Deep Convolutional GAN) называют GAN, который явно использует свёртки и транспонированные свёртки в дискриминаторе и генераторе соответственно. Откройте статью -- мы будем идти очень близко с авторами."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iYPV9p90wSgG"
      },
      "cell_type": "markdown",
      "source": [
        "## Датасет\n",
        "Всем надоели цифры, поэтому обучаться мы будем на датасете CelebA ([Large-scale CelebFaces Attributes](Large-scale CelebFaces Attributes)). В датасете на каждую фотку есть её аттрибуты, но мы их пока использовать не будем.\n",
        "\n",
        "<img width='500px' src='http://mmlab.ie.cuhk.edu.hk/projects/celeba/overview.png'>\n",
        "\n",
        "Автор, когда готовил эту тетрадку, долго думал, как загрузить датасет, чтобы всем было удобно. Это оказалось трудно, потому что прямых ссылок на него нигде нет, и, соответственно, просто сделать `!wget ...` нельзя. По удачному стечению обстоятельств, неделю назад кто-то [добавил](https://github.com/pytorch/vision/blob/master/torchvision/datasets/celeba.py) скрипты для загрузки этого датасета в сам `torchvision`, но в `pip` новая версия за такой срок ещё не успела появиться, поэтому мы обновимся напрямую из репозитория на гитхабе:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "roFGEo3b37Wa",
        "outputId": "110cb32d-a317-4a23-fbb3-0a07e62de6e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/pytorch/vision.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/pytorch/vision.git\n",
            "  Cloning https://github.com/pytorch/vision.git to /tmp/pip-req-build-4jsvmb89\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.3) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.3) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.3) (1.0.1.post2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.3) (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.2.3) (0.46)\n",
            "Building wheels for collected packages: torchvision\n",
            "  Building wheel for torchvision (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-wf8whmgp/wheels/04/6d/bf/cc14a58bae32d07d1c7d23833dc5ea655e477ff25061b8cd57\n",
            "Successfully built torchvision\n",
            "\u001b[31mfastai 1.0.50.post1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torchvision\n",
            "  Found existing installation: torchvision 0.2.2.post3\n",
            "    Uninstalling torchvision-0.2.2.post3:\n",
            "      Successfully uninstalled torchvision-0.2.2.post3\n",
            "Successfully installed torchvision-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z7AF9qQ04b2k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "import torch.optim as optim\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "raBYoUEjeXRm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')  # не забудьте включить GPU\n",
        "\n",
        "image_size = 64\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "__rYnOod4ded",
        "outputId": "1437821d-4e9c-4f72-d7dd-3caa979ede01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    # Normalize здесь приводит значения в промежуток [-1, 1]\n",
        "])\n",
        "\n",
        "dataset = datasets.CelebA('data', download=True, transform=transform)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1443490838it [00:10, 133328052.48it/s]\n",
            "26721026it [00:00, 153760395.77it/s]\n",
            "3424458it [00:00, 133707100.78it/s]\n",
            "6082035it [00:00, 80999249.79it/s]\n",
            "12156055it [00:00, 79995152.10it/s]\n",
            "2836386it [00:00, 157525822.22it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BE8y2oIB_2N3",
        "outputId": "8f5456c2-7855-4c5e-ad4c-f5e71e03f1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "cell_type": "code",
      "source": [
        "dataset[5][0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         ...,\n",
              "         [ 0.9843,  0.9843,  0.9922,  ...,  0.3333,  0.6706,  0.7725],\n",
              "         [ 0.9686,  0.9686,  0.9765,  ...,  0.1451,  0.4431,  0.5843],\n",
              "         [ 0.9922,  0.9922,  0.9843,  ..., -0.0902,  0.1059,  0.2078]],\n",
              "\n",
              "        [[ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         ...,\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.3255,  0.6784,  0.7490],\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.1137,  0.4275,  0.5686],\n",
              "         [ 0.9922,  0.9922,  0.9922,  ..., -0.1373,  0.0824,  0.1922]],\n",
              "\n",
              "        [[ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         [ 1.0000,  1.0000,  1.0000,  ...,  0.8118,  0.8039,  0.7961],\n",
              "         ...,\n",
              "         [ 1.0000,  0.9922,  0.9843,  ...,  0.2471,  0.6706,  0.7725],\n",
              "         [ 1.0000,  0.9843,  0.9451,  ...,  0.0353,  0.3961,  0.5765],\n",
              "         [ 1.0000,  0.9843,  0.9686,  ..., -0.1922,  0.0745,  0.2235]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iLSFZAaNe9NA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# посмотрите на данные (вы писали нужный код в колоризации)\n",
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hv2Jga9EPaeT"
      },
      "cell_type": "markdown",
      "source": [
        "## Модель\n",
        "\n",
        "Генератор $G$ преобразует латентный вектор $z$ в пространство данных (в нашем случае -- картинки 3x64x64). В статье используют последовательность блоков из транспонированных свёрток, BatchNorm-ов и ReLU. На выходе каждое значение лежит в [-1, 1] (мы делаем TanH), в соответствии с нормализацией, которую мы сделали раньше.\n",
        "\n",
        "<img width='600px' src='https://pytorch.org/tutorials/_images/dcgan_generator.png'>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xUkaAeWKAYpX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3CuKv_Zi54ra",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_channels = 3\n",
        "latent_size = 100\n",
        "base_size = 64\n",
        "\n",
        "G = nn.Sequential(\n",
        "    # input is Z, going into a convolution\n",
        "    nn.ConvTranspose2d(latent_size, base_size * 8, 4, 1, 0, bias=False),\n",
        "    nn.BatchNorm2d(base_size * 8),\n",
        "    nn.ReLU(True),\n",
        "    \n",
        "    # (base_size*8) x 4 x 4\n",
        "    nn.ConvTranspose2d(base_size * 8, base_size * 4, 4, 2, 1, bias=False),\n",
        "    nn.BatchNorm2d(base_size * 4),\n",
        "    nn.ReLU(True),\n",
        "    \n",
        "    # (base_size*4) x 8 x 8\n",
        "    nn.ConvTranspose2d(base_size * 4, base_size * 2, 4, 2, 1, bias=False),\n",
        "    nn.BatchNorm2d(base_size * 2),\n",
        "    nn.ReLU(True),\n",
        "    \n",
        "    # (base_size*2) x 16 x 16\n",
        "    nn.ConvTranspose2d(base_size * 2, base_size, 4, 2, 1, bias=False),\n",
        "    nn.BatchNorm2d(base_size),\n",
        "    nn.ReLU(True),\n",
        "    \n",
        "    # (base_size) x 32 x 32\n",
        "    nn.ConvTranspose2d(base_size, num_channels, 4, 2, 1, bias=False),\n",
        "    nn.Tanh()\n",
        "    # (num_channels) x 64 x 64\n",
        ").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wtv1uteaAcjZ",
        "outputId": "90f0664a-cc14-433c-f9c4-0a136b38faf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "z = torch.randn(1, latent_size, 1, 1)\n",
        "G(z).shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-0c7SaZsidzx"
      },
      "cell_type": "markdown",
      "source": [
        "Дискриминатор -- это обычный бинарный классификатор. В статье он устроен симметрично генератору: Conv2d, BatchNorm, ReLU, Conv2d..."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LZdLcZzviZlD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "D = nn.Sequential(\n",
        "\n",
        "    nn.Conv2d(num_channels, base_size , 4, 2, 1, bias=False),\n",
        "    nn.BatchNorm2d(base_size),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    \n",
        "    nn.Conv2d(base_size, base_size * 2, 4, 2, 1, bias=False),\n",
        "    nn.BatchNorm2d(base_size * 2),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # state size. (ndf*2) x 16 x 16\n",
        "    nn.Conv2d(base_size * 2, base_size * 4, 4, 2, 1, bias=False),\n",
        "    nn.BatchNorm2d(base_size * 4),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # state size. (ndf*4) x 8 x 8\n",
        "    nn.Conv2d(base_size * 4, base_size * 8, 4, 2, 1, bias=False),\n",
        "    nn.BatchNorm2d(base_size * 8),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # state size. (ndf*8) x 4 x 4\n",
        "    nn.Conv2d(base_size * 8, 1, 4, 1, 0, bias=False),\n",
        "    nn.Sigmoid()\n",
        ").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RF905ABD9cTG",
        "outputId": "3d105e5f-7ff1-4933-f699-a71df10333ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "z = torch.randn(64, latent_size, 1, 1).to(device)\n",
        "D(G(z)).shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "EFSpwNAiLG30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b56a7f5-e2b5-4d99-f18a-8aeea7fa0d89"
      },
      "cell_type": "code",
      "source": [
        "for i,_ in loader:\n",
        "  print(D(i).shape)\n",
        "  break"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_cvC_RdKiMmS"
      },
      "cell_type": "markdown",
      "source": [
        "В статье акцентируют внимание на необходимость нестандартной инициализации весов."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JTR8nHzu77kZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "035a83f9-836b-44f7-ca51-e30fd1e7fef9"
      },
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "# apply рекурсивно применяет применяет функцию ко всем своим подмодулям\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): LeakyReLU(negative_slope=0.2, inplace)\n",
              "  (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (5): LeakyReLU(negative_slope=0.2, inplace)\n",
              "  (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (8): LeakyReLU(negative_slope=0.2, inplace)\n",
              "  (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): LeakyReLU(negative_slope=0.2, inplace)\n",
              "  (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "  (13): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1NV6gwpojEv4"
      },
      "cell_type": "markdown",
      "source": [
        "## Обучение\n",
        "\n",
        "У GANов, помимо сходимости, есть проблема, что их непонятно, как сравнивать между собой, потому что у нас не один лосс, а два. Поэтому полезнее во время обучения смотреть на генерируемые картинки, а не цифры."
      ]
    },
    {
      "metadata": {
        "id": "6cLuBenQIY9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_numpy_image(img):\n",
        "    img=img.detach().cpu().view(64, 64).numpy()\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01SQM_rrG24o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "08cf46cb-4145-41ae-ec28-448729ff45f6"
      },
      "cell_type": "code",
      "source": [
        "fixed_noise=torch.randn(1, latent_size, 1, 1).to(device)\n",
        "batch_size =64\n",
        "\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-3\n",
        "\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "optimizerD = optim.Adam(D.parameters(), lr=learning_rate)\n",
        "optimizerG = optim.Adam(G.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "i=0\n",
        "for epoch in range(num_epochs):\n",
        "    for (data, _) in loader:\n",
        "        i+=1\n",
        "\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "\n",
        "        # train with real\n",
        "        D.zero_grad()\n",
        "#         real_cpu = data[0].to(device)\n",
        "        \n",
        "        \n",
        "        label = torch.full((batch_size,), 1, device=device)\n",
        "        label.fill_(1)\n",
        "        output = D(data)\n",
        "        D_loss1 = criterion(output, label)\n",
        "        D_loss1.backward()\n",
        "\n",
        "        # train with fake\n",
        "        z = torch.randn(batch_size, latent_size, 1, 1)\n",
        "        fake_photo = G(z)\n",
        "        label.fill_(0)\n",
        "        output = D(fake_photo.detach())\n",
        "        D_loss2 = criterion(output, label)\n",
        "        D_loss2.backward()\n",
        "\n",
        "        D_loss = D_loss1 + D_loss2\n",
        "        optimizerD.step()\n",
        "\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "\n",
        "        G.zero_grad()\n",
        "        label.fill_(1)  # fake labels are real for generator cost\n",
        "        output = D(fake_photo)\n",
        "        G_loss = criterion(output, label)\n",
        "        G_loss.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            # Выведем информацию о том, как наша сеть справляется\n",
        "            print(f'{epoch}/{num_epochs}, {i/len(loader)}')\n",
        "            print(f'  G loss: {G_loss}')\n",
        "            print(f'  D loss: {D_loss}')\n",
        "            print()\n",
        "            \n",
        "        if i % 50 == 0:\n",
        "            fake = G(fixed_noise)\n",
        "            img_list.append(fake)\n",
        "            img_pred = to_numpy_image(fake)\n",
        "\n",
        "            plt.figure(figsize=(10,10))\n",
        "            plt.imshow(img_pred)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BHXRu3cN8--y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "5ddabf96-0993-49b9-c6fe-84cd1cccd93b"
      },
      "cell_type": "code",
      "source": [
        "img_pred = to_numpy_image(img_list[0])\n",
        "    # теперь это numpy-евский ndarray размера (128, 128, 3)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img_pred)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c2400edd9b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_numpy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# теперь это numpy-евский ndarray размера (128, 128, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "53H3JFfJveS2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(G_losses, label=\"G\")\n",
        "plt.plot(D_losses, label=\"D\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jP4hziIekgfg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# распечатайте ваши картинки"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6kUdCbvsZgXq"
      },
      "cell_type": "markdown",
      "source": [
        "### Что дальше?\n",
        "\n",
        "Довольно старый, но актуальный список трюков: https://github.com/soumith/ganhacks\n",
        "\n",
        "Вообще, теория сходимости GANов очень сильно развилась за последнее время. Если хотите во всём этом разобраться, то возьмите какую-нибудь [достаточно новую статью](https://arxiv.org/pdf/1802.05957.pdf) и рекурсивно почитайте оттуда абстракты из списока литературы."
      ]
    }
  ]
}