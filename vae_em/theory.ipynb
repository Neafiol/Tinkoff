{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Neafiol/Tinkoff/blob/master/theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CS6-LOzZIOkS"
   },
   "source": [
    "# Контрольные вопросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s5CG7_IIOkT"
   },
   "source": [
    "1. Приведите 2 примера слоёв (и как конкретно они работают), поведение которых отличается во время обучения и инференса.\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "- dropout, во время обучения его коэффициенты равны 0|1, во время предсказания всегда 1\n",
    "- batch normal, во время обучения он перевычесляет _median_ и _b_ на каждом проходе, во время предсказания использует сохраненные константы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nE-MHEWIOkU"
   },
   "source": [
    "2. Зачем нужен батч? Почему бы не использовать весь датасет? Почему бы не использовать один пример? Почему бы не прогнать сеть на $n$ примерах независимо, а затем усреднить градиенты параметров?\n",
    "\n",
    "**Ответ:**\n",
    "- Бач нужен что бы ускорить обучение и сделать изменение градиента более плавным.\n",
    "- Если просчитать loss для всего датасета и потом сделать шаг оптимизатора то мы просто сделаем 1 максимально усредненный шаг в каком-то направлении и можем не изучить особенности связаные с конкреиными случаями в данных, так же обучение на всем датасете требует очень больших ресурсов. \n",
    "- Прогнав модель на одном примере мы вообще ничего не сможем обучить так  как никаких зависимостей данных - результатов нет, даже разгых результатов нет, модель просто научится выводить константу.\n",
    "- Высчитывать градиент после каждого прогона слишком затратно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y52cxSfxIOkV"
   },
   "source": [
    "3. Что не так с этим примером кода?\n",
    "\n",
    "```python\n",
    "history = []\n",
    "for X, Y in dataloader:\n",
    "    preds = model(X)\n",
    "    \n",
    "    loss = criterion(preds, Y)\n",
    "    loss.backward()\n",
    "    \n",
    "    history.append(loss)\n",
    "```\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "Смотря что хотел автор), если обучить модель, то он забыл `model.backward()` и `model.zero_grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgVP5lTEIOkW"
   },
   "source": [
    "4. Почему не имеет смысл использовать два линейных слоя подряд? \n",
    "\n",
    "**Ответ:**\n",
    "- При перемножение весов  2ух слоев каждый нейрон все все равно прсто приобрет какой - то вес и смещение, которого можно добиться используя только один слой, следовательно 2 Liner подряд - просто пустая трата ресурсов компьютера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TujYSuRqIOkX"
   },
   "source": [
    "5. Почему люди отказались от сигмоиды как от функции активации? Какие у неё недостатки?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "Она нечувствительна к ненормированым данным, все большые числа -> 1, маленикие к 0 в результате она может плохо различать разные случаи с большим значениями на выходе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bf3xSW97IOkY"
   },
   "source": [
    "6. Параметры сети весят 100MB. Сколько дополнительной памяти потребуется на хранение параметров, если при обучении использовать Adam?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "- 196.34 Мб"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LqV69AvIOka"
   },
   "source": [
    "7. Чем латентные переменные отличаются от параметров модели? На примере задачи кластеризации (k-means), что является латентными переменными, а что параметрами?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "- латентные переменные подбираются и изменяются во время обучения, в то время как параметры это костнанты, закрепленные во время инициализации модели\n",
    "\n",
    "- параметр : `n_clusters` - количество классов, латентные переменные : U$k$ - центры кластеров элементов, принадлежащих одному классу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0H49voZIOkb"
   },
   "source": [
    "8. Как использовать VAE для генерации цифр MNIST конкретного класса? Разрешается обучить только одну сеть.\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "- добавить между `encoder` и `decoder`2 паралельных и 1 последовательный `Linear()` в которых реализовать оценку близости нейронов, кодирующих одинаковые цифры на скрытом слое.\n",
    "                                            ↓\n",
    "`torch.randn(nn.Linear(h_dim, z_dim).size())`   *   `nn.Linear(h_dim, z_dim).mul(0.5).exp_()` + `nn.Linear(h_dim, z_dim)`\n",
    "                                            ↓\n",
    "                             self.z2h=nn.Linear(z_dim,h_dim)\n",
    "                                            ↓\n",
    "                                           ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YE4ShzvvIOkd"
   },
   "source": [
    "9. Как сравнить качество двух разных GAN-ов, обученых на ImageNet?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "- Глазами\n",
    "- Обучить модель, которая должна определять реальная перед ней картика или сгенерированная"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4LcxLziIOkf"
   },
   "source": [
    "10. (Сложно) Пусть есть сеть из $n$ линейных слоев одинакового размера с сигмоидой в качестве функции активации после каждого слоя. Обычно, мы сохраняем выход каждого слоя, из-за чего нам требуется $O(n)$ дополнительной памяти на каждый пример в батче. Можно ли обучить такую сеть, использая $O(1)$ дополнительной памяти, оставив асимптотику по времени $O(n)$?\n",
    "\n",
    "**Ответ:**\n",
    "\n",
    "- можно не сохранять, а передовать результат проеобразования каждого слоя по ссылке, таким образом на нужно будет хранить только 1 матрицу и т.к. размер всех слоев одинаковый ее размер не будет меняться во время прохода по nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "theory.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
