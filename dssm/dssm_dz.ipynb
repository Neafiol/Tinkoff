{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dssm + dz.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "RAmhaEaFzvD2"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neafiol/Tinkoff/blob/master/dssm/dssm_dz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNxseYLzzvCv",
        "colab_type": "text"
      },
      "source": [
        "# Seminar: simple question answering\n",
        "![img](https://recruitlook.com/wp-content/uploads/2015/01/questionanswer3.jpg)\n",
        "\n",
        "Today we're going to build a retrieval-based question answering model with metric learning models.\n",
        "\n",
        "_this seminar is based on original notebook by [Oleg Vasilev](https://github.com/Omrigan/)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUxx5lPpzvCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5tlHnRNz1gh",
        "colab_type": "code",
        "outputId": "de3706e9-ad6e-4f3c-97f3-a024e9689353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/week11_dssm/utils.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-04 18:33:41--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/week11_dssm/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9814 (9.6K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   9.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-05-04 18:33:47 (107 MB/s) - ‘utils.py’ saved [9814/9814]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Rp5Msv0BNf",
        "colab_type": "code",
        "outputId": "f958a7fc-6ad7-4160-ef88-427f71664583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxgOGXMbzvC2",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Today's data is Stanford Question Answering Dataset (SQuAD). Given a paragraph of text and a question, our model's task is to select a snippet that answers the question.\n",
        "\n",
        "We are not going to solve the full task today. Instead, we'll train a model to __select the sentence containing answer__ among several options.\n",
        "\n",
        "As usual, you are given an utility module with data reader and some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US39zXJlzvC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import utils\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad-v2.0.json 2> log\n",
        "# backup download link: https://www.dropbox.com/s/q4fuihaerqr0itj/squad.tar.gz?dl=1\n",
        "train, test = utils.build_dataset('./squad-v2.0.json', tokenized=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF7j-vCpzvC6",
        "colab_type": "code",
        "outputId": "94c74138-4895-4aee-cdb3-26b870656551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# the data comes pre-tokenized with this simple tokenizer:\n",
        "utils.tokenize(\"I... I'm the monument to all your sins.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i ... i ' m the monument to all your sins .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNr15FbFzvC9",
        "colab_type": "code",
        "outputId": "6ce8baf5-7da6-458b-b80c-6a498cab519a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "pid, question, options, correct_indices, wrong_indices = train.iloc[40]\n",
        "print('QUESTION', question, '\\n')\n",
        "for i, cand in enumerate(options):\n",
        "    print(['[ ]', '[v]'][i in correct_indices], cand)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QUESTION where did beyonce get her name from ? \n",
            "\n",
            "[ ] beyoncé giselle knowles was born in houston , texas , to celestine ann \" tina \" knowles ( née beyincé ), a hairdresser and salon owner , and mathew knowles , a xerox sales manager .\n",
            "[v] beyoncé ' s name is a tribute to her mother ' s maiden name .\n",
            "[ ] beyoncé ' s younger sister solange is also a singer and a former member of destiny ' s child .\n",
            "[ ] mathew is african - american , while tina is of louisiana creole descent ( with african , native american , french , cajun , and distant irish and spanish ancestry ).\n",
            "[ ] through her mother , beyoncé is a descendant of acadian leader joseph broussard .\n",
            "[ ] she was raised in a methodist household .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKnrxjsKzvDA",
        "colab_type": "text"
      },
      "source": [
        "### Tokens & vocabularies\n",
        "\n",
        "The procedure here is very similar to previous nlp weeks: preprocess text into tokens, create dictionaries, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_ir3y7AD_KR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "mat=[]\n",
        "for q in (train['question']):\n",
        "  q=q.replace(',','')\n",
        "  q=q.replace('.','')\n",
        "  q=q.replace('?','')\n",
        "  q=q.replace('`','')\n",
        "\n",
        "  for w in q.split(' '):\n",
        "    mat.append(w)\n",
        "\n",
        "for o in (train[\"options\"]):\n",
        "  for q in o:\n",
        "    q=q.replace(',','')\n",
        "    q=q.replace('.','')\n",
        "    q=q.replace('?','')\n",
        "    q=q.replace('`','')\n",
        "\n",
        "    for w in q.split(' '):\n",
        "      mat.append(w)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YV-QLcaEzXa",
        "colab_type": "code",
        "outputId": "a7a4142f-db46-4718-e467-a8da65a47b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(mat)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9042915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oh7zJoJzvDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm, trange\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "#Dictionary of {token : count}\n",
        "token_counts = Counter(mat)\n",
        "\n",
        "# compute counts for each token; use token_counts;\n",
        "# count BOTH in train['question'] and in train['options']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2I4L6lOE9J1",
        "colab_type": "code",
        "outputId": "fd0c9d67-592a-454a-be7d-892ca878bd68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "token_counts['me']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miZzEh1UzvDG",
        "colab_type": "code",
        "outputId": "a37c1d67-e2f0-432c-fef8-0a18025625ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Total tokens:\", sum(token_counts.values()))\n",
        "print(\"Most common:\", token_counts.most_common(5))\n",
        "assert 9000000 < sum(token_counts.values()) < 9100000, \"are you sure you counted all unique tokens in questions and options?\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total tokens: 9042915\n",
            "Most common: [('', 802262), ('the', 597790), ('of', 300056), ('and', 231619), ('in', 215312)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlw19UFFzvDJ",
        "colab_type": "text"
      },
      "source": [
        "We shall only keep tokens that are present at least 4 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xf3oSHqHVJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# token_counts=sorted(token_counts, key=lambda k: token_counts[k])\n",
        "# token_counts.reverse()\n",
        "tokens=token_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNvC_4tDzvDK",
        "colab_type": "code",
        "outputId": "a253a8f4-3983-48fc-a839-dcc267573a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MIN_COUNT = 5\n",
        "\n",
        "tokens = [c for c in token_counts if token_counts[c] >  MIN_COUNT] \n",
        "tokens = [\"_PAD_\", \"_UNK_\"] + tokens\n",
        "print(\"Tokens left:\", len(tokens))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens left: 40801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9NK0qpRzvDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a dictionary from token to it's index in tokens\n",
        "token_to_id={}\n",
        "for i in range(len(tokens)) :\n",
        "  token_to_id[tokens[i]] = i "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qoe01vGI3j5",
        "colab_type": "code",
        "outputId": "73adad93-f9ce-4c95-bb74-da69adbbf48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "token_to_id[\"the\"]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAWawL47zvDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert token_to_id['me'] != token_to_id['woods']\n",
        "assert token_to_id[tokens[42]]==42\n",
        "assert len(token_to_id)==len(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xn5NzZ0zvDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_ix = token_to_id[\"_PAD_\"]\n",
        "UNK_ix = token_to_id['_UNK_']\n",
        "\n",
        "#good old as_matrix for the third time\n",
        "def as_matrix(sequences, max_len=None):\n",
        "    if isinstance(sequences[0], (str, bytes)):\n",
        "        sequences = [utils.tokenize(s).split() for s in sequences]\n",
        "        \n",
        "    max_len = max_len or max(map(len,sequences))\n",
        "    \n",
        "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + PAD_ix\n",
        "    for i, seq in enumerate(sequences):\n",
        "        row_ix = [token_to_id.get(word, UNK_ix) for word in seq[:max_len]]\n",
        "        matrix[i, :len(row_ix)] = row_ix\n",
        "    \n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK25QWBNzvDV",
        "colab_type": "code",
        "outputId": "351f862a-969c-4982-bc45-a35ccc6bc2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test = as_matrix([\"Definitely, thOsE tokens areN'T LowerCASE!!\", \"I'm the monument to all your sins.\"])\n",
        "print(test)\n",
        "assert test.shape[0]==2\n",
        "print(\"Correct!\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11174     1  1994  7720  7406    19   972 12207     1     0]\n",
            " [  196    19  2521    37  6800    49   647  6234 24359     1]]\n",
            "Correct!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T058hedOzvDX",
        "colab_type": "text"
      },
      "source": [
        "### Data sampler\n",
        "\n",
        "Our model trains on triplets: $<query, answer^+, answer^->$\n",
        "\n",
        "For your convenience, we've implemented a function that samples such triplets from data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg4xlH_6zvDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch\n",
        "lines_to_tensor = lambda lines, max_len=None: torch.tensor(\n",
        "    as_matrix(lines, max_len=max_len), dtype=torch.int64)\n",
        "\n",
        "def iterate_minibatches(data, batch_size, shuffle=True, cycle=False):\n",
        "    \"\"\"\n",
        "    Generates minibatches of triples: {questions, correct answers, wrong answers}\n",
        "    If there are several wrong (or correct) answers, picks one at random.\n",
        "    \"\"\"\n",
        "    indices = np.arange(len(data))\n",
        "    while True:\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(indices)\n",
        "        for batch_start in range(0, len(indices), batch_size):\n",
        "            batch_indices = indices[batch_start: batch_start + batch_size]\n",
        "            batch = data.iloc[batch_indices]\n",
        "            questions = batch['question'].values\n",
        "            correct_answers = np.array([\n",
        "                row['options'][random.choice(row['correct_indices'])]\n",
        "                for i, row in batch.iterrows()\n",
        "            ])\n",
        "            wrong_answers = np.array([\n",
        "                row['options'][random.choice(row['wrong_indices'])]\n",
        "                for i, row in batch.iterrows()\n",
        "            ])\n",
        "\n",
        "            yield {\n",
        "                'questions' : lines_to_tensor(questions),\n",
        "                'correct_answers': lines_to_tensor(correct_answers),\n",
        "                'wrong_answers': lines_to_tensor(wrong_answers),\n",
        "            }\n",
        "        if not cycle:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj34W-UcMOE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1866
        },
        "outputId": "8c289939-5e27-43f5-89df-1645d2dd4302"
      },
      "source": [
        "train"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paragraph_id</th>\n",
              "      <th>question</th>\n",
              "      <th>options</th>\n",
              "      <th>correct_indices</th>\n",
              "      <th>wrong_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>when did beyonce start becoming popular ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>when did beyonce leave destiny ' s child and b...</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>in what city and state did beyonce grow up ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>in which decade did beyonce become famous ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>in what r &amp; b group was she the lead singer ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>what album made her a worldwide known artist ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>who managed the destiny ' s child group ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>when did beyoncé rise to fame ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>what role did beyoncé have in destiny ' s child ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>what was the first album beyoncé released as a...</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>when did beyoncé release dangerously in love ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>how many grammy awards did beyoncé win for her...</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>what was beyoncé ' s role in destiny ' s child ?</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>what was the name of beyoncé ' s first solo al...</td>\n",
              "      <td>[beyoncé giselle knowles - carter (/ biːˈjɒnse...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>after her second solo album , what other enter...</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>which artist did beyonce marry ?</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>to set the record for grammys , how many did b...</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>for what movie did beyonce receive her first g...</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>when did beyonce take a hiatus in her career a...</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>which album was darker in tone from her previo...</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>after what movie portraying etta james , did b...</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>when did destiny ' s child end their group act ?</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>what was the name of beyoncé ' s second solo a...</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>what was beyoncé ' s first acting job , in 2006 ?</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>who is beyoncé married to ?</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>what is the name of beyoncé ' s alter - ego ?</td>\n",
              "      <td>[following the disbandment of destiny ' s chil...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2</td>\n",
              "      <td>in her music , what are some recurring element...</td>\n",
              "      <td>[a self - described \" modern - day feminist \",...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2</td>\n",
              "      <td>time magazine named her one of the most 100 wh...</td>\n",
              "      <td>[a self - described \" modern - day feminist \",...</td>\n",
              "      <td>[6]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2</td>\n",
              "      <td>which magazine declared her the most dominant ...</td>\n",
              "      <td>[a self - described \" modern - day feminist \",...</td>\n",
              "      <td>[7]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85091</th>\n",
              "      <td>49</td>\n",
              "      <td>before hindus are cremated , how many times ar...</td>\n",
              "      <td>[the bagmati river which flows through kathman...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85092</th>\n",
              "      <td>50</td>\n",
              "      <td>in what century was bhrikuti said to live ?</td>\n",
              "      <td>[legendary princess bhrikuti ( 7th - century )...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85093</th>\n",
              "      <td>50</td>\n",
              "      <td>when did araniko die ?</td>\n",
              "      <td>[legendary princess bhrikuti ( 7th - century )...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85094</th>\n",
              "      <td>50</td>\n",
              "      <td>what religion did araniko help to evangelize ?</td>\n",
              "      <td>[legendary princess bhrikuti ( 7th - century )...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85095</th>\n",
              "      <td>50</td>\n",
              "      <td>how many newar buddhist monasteries are presen...</td>\n",
              "      <td>[legendary princess bhrikuti ( 7th - century )...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85096</th>\n",
              "      <td>50</td>\n",
              "      <td>approximately how many monasteries in the kath...</td>\n",
              "      <td>[legendary princess bhrikuti ( 7th - century )...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85097</th>\n",
              "      <td>51</td>\n",
              "      <td>what type of religion is kirant mundhum ?</td>\n",
              "      <td>[kirant mundhum is one of the indigenous animi...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85098</th>\n",
              "      <td>51</td>\n",
              "      <td>who follows the kirant mudhum faith ?</td>\n",
              "      <td>[kirant mundhum is one of the indigenous animi...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85099</th>\n",
              "      <td>51</td>\n",
              "      <td>who worshipped at wanga akash bhairabh in anci...</td>\n",
              "      <td>[kirant mundhum is one of the indigenous animi...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85100</th>\n",
              "      <td>51</td>\n",
              "      <td>what is another name for ancestor worship ?</td>\n",
              "      <td>[kirant mundhum is one of the indigenous animi...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85101</th>\n",
              "      <td>52</td>\n",
              "      <td>where can a temple of the jain faith be found ?</td>\n",
              "      <td>[sikhism is practiced primarily in gurudwara a...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85102</th>\n",
              "      <td>52</td>\n",
              "      <td>kathmandu valley is home to about how many bah...</td>\n",
              "      <td>[sikhism is practiced primarily in gurudwara a...</td>\n",
              "      <td>[4]</td>\n",
              "      <td>[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85103</th>\n",
              "      <td>52</td>\n",
              "      <td>where is the baha ' i national office located ...</td>\n",
              "      <td>[sikhism is practiced primarily in gurudwara a...</td>\n",
              "      <td>[5]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85104</th>\n",
              "      <td>52</td>\n",
              "      <td>about what percentage of the nepali population...</td>\n",
              "      <td>[sikhism is practiced primarily in gurudwara a...</td>\n",
              "      <td>[7]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85105</th>\n",
              "      <td>52</td>\n",
              "      <td>about how many christian houses of worship exi...</td>\n",
              "      <td>[sikhism is practiced primarily in gurudwara a...</td>\n",
              "      <td>[8]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85111</th>\n",
              "      <td>54</td>\n",
              "      <td>along with cricket , what sport is highly popu...</td>\n",
              "      <td>[football and cricket are the most popular spo...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85112</th>\n",
              "      <td>54</td>\n",
              "      <td>what body oversees soccer in nepal ?</td>\n",
              "      <td>[football and cricket are the most popular spo...</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[0, 2, 3, 4, 5, 6, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85113</th>\n",
              "      <td>54</td>\n",
              "      <td>how many people can fit in dasarath rangasala ...</td>\n",
              "      <td>[football and cricket are the most popular spo...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2, 4, 5, 6, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85114</th>\n",
              "      <td>54</td>\n",
              "      <td>in what part of kathmandu is dasarath rangasal...</td>\n",
              "      <td>[football and cricket are the most popular spo...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4, 5, 6, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85115</th>\n",
              "      <td>54</td>\n",
              "      <td>who assisted nepal in renovating dasarath rang...</td>\n",
              "      <td>[football and cricket are the most popular spo...</td>\n",
              "      <td>[5]</td>\n",
              "      <td>[0, 1, 2, 3, 4, 6, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85116</th>\n",
              "      <td>55</td>\n",
              "      <td>as of 2004 , how many kilometers of road exist...</td>\n",
              "      <td>[the total length of roads in nepal is recorde...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85117</th>\n",
              "      <td>55</td>\n",
              "      <td>why is travel in kathmandu mainly via automobi...</td>\n",
              "      <td>[the total length of roads in nepal is recorde...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85118</th>\n",
              "      <td>55</td>\n",
              "      <td>what highway connecting kathmandu to elsewhere...</td>\n",
              "      <td>[the total length of roads in nepal is recorde...</td>\n",
              "      <td>[4]</td>\n",
              "      <td>[0, 1, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85119</th>\n",
              "      <td>55</td>\n",
              "      <td>in what direction out of kathmandu does the pr...</td>\n",
              "      <td>[the total length of roads in nepal is recorde...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85120</th>\n",
              "      <td>55</td>\n",
              "      <td>if one wished to travel north out of kathmandu...</td>\n",
              "      <td>[the total length of roads in nepal is recorde...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85121</th>\n",
              "      <td>56</td>\n",
              "      <td>what is nepal ' s primary airport for internat...</td>\n",
              "      <td>[the main international airport serving kathma...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85122</th>\n",
              "      <td>56</td>\n",
              "      <td>starting in the center of kathmandu , how many...</td>\n",
              "      <td>[the main international airport serving kathma...</td>\n",
              "      <td>[0]</td>\n",
              "      <td>[1, 2, 3, 4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85123</th>\n",
              "      <td>56</td>\n",
              "      <td>how many airlines use tribhuvan international ...</td>\n",
              "      <td>[the main international airport serving kathma...</td>\n",
              "      <td>[2]</td>\n",
              "      <td>[0, 1, 3, 4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85124</th>\n",
              "      <td>56</td>\n",
              "      <td>from what city does arkefly offer nonstop flig...</td>\n",
              "      <td>[the main international airport serving kathma...</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[0, 1, 2, 4, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85125</th>\n",
              "      <td>56</td>\n",
              "      <td>who operates flights between kathmandu and ist...</td>\n",
              "      <td>[the main international airport serving kathma...</td>\n",
              "      <td>[4]</td>\n",
              "      <td>[0, 1, 2, 3, 5]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>58161 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       paragraph_id                                           question  \\\n",
              "0                 0          when did beyonce start becoming popular ?   \n",
              "1                 0  what areas did beyonce compete in when she was...   \n",
              "2                 0  when did beyonce leave destiny ' s child and b...   \n",
              "3                 0       in what city and state did beyonce grow up ?   \n",
              "4                 0        in which decade did beyonce become famous ?   \n",
              "5                 0      in what r & b group was she the lead singer ?   \n",
              "6                 0     what album made her a worldwide known artist ?   \n",
              "7                 0          who managed the destiny ' s child group ?   \n",
              "8                 0                    when did beyoncé rise to fame ?   \n",
              "9                 0  what role did beyoncé have in destiny ' s child ?   \n",
              "10                0  what was the first album beyoncé released as a...   \n",
              "11                0     when did beyoncé release dangerously in love ?   \n",
              "12                0  how many grammy awards did beyoncé win for her...   \n",
              "13                0   what was beyoncé ' s role in destiny ' s child ?   \n",
              "14                0  what was the name of beyoncé ' s first solo al...   \n",
              "15                1  after her second solo album , what other enter...   \n",
              "16                1                   which artist did beyonce marry ?   \n",
              "17                1  to set the record for grammys , how many did b...   \n",
              "18                1  for what movie did beyonce receive her first g...   \n",
              "19                1  when did beyonce take a hiatus in her career a...   \n",
              "20                1  which album was darker in tone from her previo...   \n",
              "21                1  after what movie portraying etta james , did b...   \n",
              "22                1   when did destiny ' s child end their group act ?   \n",
              "23                1  what was the name of beyoncé ' s second solo a...   \n",
              "24                1  what was beyoncé ' s first acting job , in 2006 ?   \n",
              "25                1                        who is beyoncé married to ?   \n",
              "26                1      what is the name of beyoncé ' s alter - ego ?   \n",
              "27                2  in her music , what are some recurring element...   \n",
              "28                2  time magazine named her one of the most 100 wh...   \n",
              "29                2  which magazine declared her the most dominant ...   \n",
              "...             ...                                                ...   \n",
              "85091            49  before hindus are cremated , how many times ar...   \n",
              "85092            50        in what century was bhrikuti said to live ?   \n",
              "85093            50                             when did araniko die ?   \n",
              "85094            50     what religion did araniko help to evangelize ?   \n",
              "85095            50  how many newar buddhist monasteries are presen...   \n",
              "85096            50  approximately how many monasteries in the kath...   \n",
              "85097            51          what type of religion is kirant mundhum ?   \n",
              "85098            51              who follows the kirant mudhum faith ?   \n",
              "85099            51  who worshipped at wanga akash bhairabh in anci...   \n",
              "85100            51        what is another name for ancestor worship ?   \n",
              "85101            52    where can a temple of the jain faith be found ?   \n",
              "85102            52  kathmandu valley is home to about how many bah...   \n",
              "85103            52  where is the baha ' i national office located ...   \n",
              "85104            52  about what percentage of the nepali population...   \n",
              "85105            52  about how many christian houses of worship exi...   \n",
              "85111            54  along with cricket , what sport is highly popu...   \n",
              "85112            54               what body oversees soccer in nepal ?   \n",
              "85113            54  how many people can fit in dasarath rangasala ...   \n",
              "85114            54  in what part of kathmandu is dasarath rangasal...   \n",
              "85115            54  who assisted nepal in renovating dasarath rang...   \n",
              "85116            55  as of 2004 , how many kilometers of road exist...   \n",
              "85117            55  why is travel in kathmandu mainly via automobi...   \n",
              "85118            55  what highway connecting kathmandu to elsewhere...   \n",
              "85119            55  in what direction out of kathmandu does the pr...   \n",
              "85120            55  if one wished to travel north out of kathmandu...   \n",
              "85121            56  what is nepal ' s primary airport for internat...   \n",
              "85122            56  starting in the center of kathmandu , how many...   \n",
              "85123            56  how many airlines use tribhuvan international ...   \n",
              "85124            56  from what city does arkefly offer nonstop flig...   \n",
              "85125            56  who operates flights between kathmandu and ist...   \n",
              "\n",
              "                                                 options correct_indices  \\\n",
              "0      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "1      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "2      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [3]   \n",
              "3      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "4      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "5      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "6      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [3]   \n",
              "7      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [2]   \n",
              "8      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "9      [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "10     [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [3]   \n",
              "11     [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [3]   \n",
              "12     [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [3]   \n",
              "13     [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [1]   \n",
              "14     [beyoncé giselle knowles - carter (/ biːˈjɒnse...             [3]   \n",
              "15     [following the disbandment of destiny ' s chil...             [1]   \n",
              "16     [following the disbandment of destiny ' s chil...             [2]   \n",
              "17     [following the disbandment of destiny ' s chil...             [2]   \n",
              "18     [following the disbandment of destiny ' s chil...             [1]   \n",
              "19     [following the disbandment of destiny ' s chil...             [2]   \n",
              "20     [following the disbandment of destiny ' s chil...             [1]   \n",
              "21     [following the disbandment of destiny ' s chil...             [2]   \n",
              "22     [following the disbandment of destiny ' s chil...             [0]   \n",
              "23     [following the disbandment of destiny ' s chil...             [0]   \n",
              "24     [following the disbandment of destiny ' s chil...             [1]   \n",
              "25     [following the disbandment of destiny ' s chil...             [2]   \n",
              "26     [following the disbandment of destiny ' s chil...             [2]   \n",
              "27     [a self - described \" modern - day feminist \",...             [0]   \n",
              "28     [a self - described \" modern - day feminist \",...             [6]   \n",
              "29     [a self - described \" modern - day feminist \",...             [7]   \n",
              "...                                                  ...             ...   \n",
              "85091  [the bagmati river which flows through kathman...             [2]   \n",
              "85092  [legendary princess bhrikuti ( 7th - century )...             [0]   \n",
              "85093  [legendary princess bhrikuti ( 7th - century )...             [0]   \n",
              "85094  [legendary princess bhrikuti ( 7th - century )...             [0]   \n",
              "85095  [legendary princess bhrikuti ( 7th - century )...             [1]   \n",
              "85096  [legendary princess bhrikuti ( 7th - century )...             [2]   \n",
              "85097  [kirant mundhum is one of the indigenous animi...             [0]   \n",
              "85098  [kirant mundhum is one of the indigenous animi...             [1]   \n",
              "85099  [kirant mundhum is one of the indigenous animi...             [3]   \n",
              "85100  [kirant mundhum is one of the indigenous animi...             [2]   \n",
              "85101  [sikhism is practiced primarily in gurudwara a...             [3]   \n",
              "85102  [sikhism is practiced primarily in gurudwara a...             [4]   \n",
              "85103  [sikhism is practiced primarily in gurudwara a...             [5]   \n",
              "85104  [sikhism is practiced primarily in gurudwara a...             [7]   \n",
              "85105  [sikhism is practiced primarily in gurudwara a...             [8]   \n",
              "85111  [football and cricket are the most popular spo...             [0]   \n",
              "85112  [football and cricket are the most popular spo...             [1]   \n",
              "85113  [football and cricket are the most popular spo...             [3]   \n",
              "85114  [football and cricket are the most popular spo...             [2]   \n",
              "85115  [football and cricket are the most popular spo...             [5]   \n",
              "85116  [the total length of roads in nepal is recorde...             [0]   \n",
              "85117  [the total length of roads in nepal is recorde...             [2]   \n",
              "85118  [the total length of roads in nepal is recorde...             [4]   \n",
              "85119  [the total length of roads in nepal is recorde...             [3]   \n",
              "85120  [the total length of roads in nepal is recorde...             [3]   \n",
              "85121  [the main international airport serving kathma...             [0]   \n",
              "85122  [the main international airport serving kathma...             [0]   \n",
              "85123  [the main international airport serving kathma...             [2]   \n",
              "85124  [the main international airport serving kathma...             [3]   \n",
              "85125  [the main international airport serving kathma...             [4]   \n",
              "\n",
              "                             wrong_indices  \n",
              "0                                [0, 2, 3]  \n",
              "1                                [0, 2, 3]  \n",
              "2                                [0, 1, 2]  \n",
              "3                                [0, 2, 3]  \n",
              "4                                [0, 2, 3]  \n",
              "5                                [0, 2, 3]  \n",
              "6                                [0, 1, 2]  \n",
              "7                                [0, 1, 3]  \n",
              "8                                [0, 2, 3]  \n",
              "9                                [0, 2, 3]  \n",
              "10                               [0, 1, 2]  \n",
              "11                               [0, 1, 2]  \n",
              "12                               [0, 1, 2]  \n",
              "13                               [0, 2, 3]  \n",
              "14                               [0, 1, 2]  \n",
              "15                            [0, 2, 3, 4]  \n",
              "16                            [0, 1, 3, 4]  \n",
              "17                            [0, 1, 3, 4]  \n",
              "18                            [0, 2, 3, 4]  \n",
              "19                            [0, 1, 3, 4]  \n",
              "20                            [0, 2, 3, 4]  \n",
              "21                            [0, 1, 3, 4]  \n",
              "22                            [1, 2, 3, 4]  \n",
              "23                            [1, 2, 3, 4]  \n",
              "24                            [0, 2, 3, 4]  \n",
              "25                            [0, 1, 3, 4]  \n",
              "26                            [0, 1, 3, 4]  \n",
              "27                   [1, 2, 3, 4, 5, 6, 7]  \n",
              "28                   [0, 1, 2, 3, 4, 5, 7]  \n",
              "29                   [0, 1, 2, 3, 4, 5, 6]  \n",
              "...                                    ...  \n",
              "85091                         [0, 1, 3, 4]  \n",
              "85092                            [1, 2, 3]  \n",
              "85093                            [1, 2, 3]  \n",
              "85094                            [1, 2, 3]  \n",
              "85095                            [0, 2, 3]  \n",
              "85096                            [0, 1, 3]  \n",
              "85097                         [1, 2, 3, 4]  \n",
              "85098                         [0, 2, 3, 4]  \n",
              "85099                         [0, 1, 2, 4]  \n",
              "85100                         [0, 1, 3, 4]  \n",
              "85101  [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11]  \n",
              "85102  [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11]  \n",
              "85103  [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11]  \n",
              "85104  [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11]  \n",
              "85105  [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11]  \n",
              "85111                [1, 2, 3, 4, 5, 6, 7]  \n",
              "85112                [0, 2, 3, 4, 5, 6, 7]  \n",
              "85113                [0, 1, 2, 4, 5, 6, 7]  \n",
              "85114                [0, 1, 3, 4, 5, 6, 7]  \n",
              "85115                [0, 1, 2, 3, 4, 6, 7]  \n",
              "85116                         [1, 2, 3, 4]  \n",
              "85117                         [0, 1, 3, 4]  \n",
              "85118                         [0, 1, 2, 3]  \n",
              "85119                         [0, 1, 2, 4]  \n",
              "85120                         [0, 1, 2, 4]  \n",
              "85121                      [1, 2, 3, 4, 5]  \n",
              "85122                      [1, 2, 3, 4, 5]  \n",
              "85123                      [0, 1, 3, 4, 5]  \n",
              "85124                      [0, 1, 2, 4, 5]  \n",
              "85125                      [0, 1, 2, 3, 5]  \n",
              "\n",
              "[58161 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuWxum9izvDZ",
        "colab_type": "code",
        "outputId": "541398c4-a910-48ae-8097-81f9af75b7b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "dummy_batch = next(iterate_minibatches(train.sample(10), 2))\n",
        "print(dummy_batch[\"questions\"])\n",
        "print(dummy_batch[\"correct_answers\"])\n",
        "print(dummy_batch[\"wrong_answers\"])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   2,    3,   37,  336,    1,   20,    1, 9675,  477, 1152,   12, 4091,\n",
            "          353,   98,    1],\n",
            "        [   4,  493,   49,   30,   44,   64,   39,   66,   37,  149,    1,    0,\n",
            "            0,    0,    0]])\n",
            "tensor([[   37,   336,     1,    20,     1,  9675,   477,  1152,    12,  4091,\n",
            "           353,  1906,    12,   996,   381,     1,   695,    49,    37,   336,\n",
            "             1,    20,     1,   455,  2721,    66,  3356,   817,  4245,     1,\n",
            "          4246,    22,    37,  2203,  5278,  8014,    49,    52,  1906,   446,\n",
            "            37,   620,   115,     1],\n",
            "        [   13,    14,   394,    64,    39,    66,    37,   149,   584,    37,\n",
            "           251,    14,   155,   329, 25970,    64,   229,  8684,  4584,    39,\n",
            "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]])\n",
            "tensor([[   37,   787,   788,  2203,  5278,  2989,  6285,  8620,    37,  5278,\n",
            "            49,   370,     1,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [  206,    37,   487,   266,    61,    62,    12,   700,   137,     1,\n",
            "            47,    14,   394,    64,   406,    62,     1,  4431,  4755,   496,\n",
            "           962,   252,    33,    34,    35,   253,    22,   252,    33,    34,\n",
            "            35,   203,    64,   254, 25965,    12,    58,     1,    22,   252,\n",
            "         20232,   263,    39,    64,    47,     1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBHPzOdZzvDb",
        "colab_type": "text"
      },
      "source": [
        "### Building the model (3 points)\n",
        "\n",
        "Our goal for today is to build a model that measures similarity between question and answer. In particular, it maps both question and answer into fixed-size vectors such that:\n",
        "\n",
        "Our model is a pair of $V_q(q)$ and $V_a(a)$ - networks that turn phrases into vectors. \n",
        "\n",
        "__Objective:__ Question vector $V_q(q)$ should be __closer__ to correct answer vectors $V_a(a^+)$ than to incorrect ones $V_a(a^-)$ .\n",
        "\n",
        "Both vectorizers can be anything you wish. For starters, let's use a convolutional network with global pooling and a couple of dense layers on top.\n",
        "\n",
        "It is perfectly legal to share some layers between vectorizers, but make sure they are at least a little different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HItbkJWNzvDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class GlobalMaxPooling(nn.Module):\n",
        "    def __init__(self, dim=-1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.dim = dim\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x.max(dim=self.dim)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dOYRxYlzvDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we might as well create a global embedding layer here\n",
        "\n",
        "GLOBAL_EMB = nn.Embedding(len(tokens), 64, padding_idx=PAD_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO0LtQmUzvDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestionVectorizer(nn.Module):\n",
        "\n",
        "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
        "        \"\"\" \n",
        "        A simple sequential encoder for questions.\n",
        "        Use any combination of layers you want to encode a variable-length input \n",
        "        to a fixed-size output vector\n",
        "        \n",
        "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
        "        \"\"\"\n",
        "        super(self.__class__, self).__init__()\n",
        "        \n",
        "        if use_global_emb:\n",
        "            self.emb = GLOBAL_EMB\n",
        "        else:\n",
        "            self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_ix)\n",
        "            \n",
        "        \n",
        "        self.rnn = nn.RNN(64, out_size, num_layers =1 , dropout=0.05)\n",
        "        \n",
        "    def forward(self, text_ix):\n",
        "        \"\"\"\n",
        "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
        "        :returns: float32 Variable of shape [batch_size, out_size]\n",
        "        \"\"\"\n",
        "#         self.inp = nn.Linear(1, hidden_size)\n",
        "       \n",
        "        e = self.emb(text_ix)\n",
        "        e = torch.transpose(e,1,0)\n",
        "        \n",
        "        h_0 = torch.zeros(e.shape)\n",
        "      \n",
        "        output, hn = self.rnn(e)\n",
        "#         self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        return output[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLdm9gekzvDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AnswerVectorizer(nn.Module):\n",
        "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
        "        \"\"\" \n",
        "        A simple sequential encoder for questions.\n",
        "        Use any combination of layers you want to encode a variable-length input \n",
        "        to a fixed-size output vector\n",
        "        \n",
        "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
        "        \"\"\"\n",
        "        super(self.__class__, self).__init__()\n",
        "        \n",
        "        if use_global_emb:\n",
        "            self.emb = GLOBAL_EMB\n",
        "        else:\n",
        "            self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_ix)\n",
        "            \n",
        "        \n",
        "        self.rnn = nn.RNN(64, out_size, num_layers =1 , dropout=0.05)\n",
        "        \n",
        "    def forward(self, text_ix):\n",
        "        \"\"\"\n",
        "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
        "        :returns: float32 Variable of shape [batch_size, out_size]\n",
        "        \"\"\"\n",
        "#         self.inp = nn.Linear(1, hidden_size)\n",
        "       \n",
        "        e = self.emb(text_ix)\n",
        "        e = torch.transpose(e,1,0)\n",
        "        \n",
        "        h_0 = torch.zeros(e.shape)\n",
        "      \n",
        "        output, hn = self.rnn(e)\n",
        "#         self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        return output[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKR9vP5arJ0g",
        "colab_type": "code",
        "outputId": "0f381ea1-082f-4f22-9011-04284af0af7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "q=AnswerVectorizer(out_size=100)\n",
        "q(torch.LongTensor(test)).shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e:  torch.Size([10, 2, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm7FDKkzzvDh",
        "colab_type": "code",
        "outputId": "eeb24fa9-ca8c-4d67-c741-35d2a2b6db67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "for vectorizer in [QuestionVectorizer(out_size=100), AnswerVectorizer(out_size=100)]:\n",
        "  \n",
        "    print(\"Testing %s ...\" % vectorizer.__class__.__name__)\n",
        "    dummy_x = torch.LongTensor(test)\n",
        "    dummy_v = vectorizer(dummy_x)\n",
        "    assert tuple(dummy_v.shape) == (dummy_x.shape[0], 100)\n",
        "\n",
        "    del vectorizer\n",
        "    print(\"Seems fine\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing QuestionVectorizer ...\n",
            "e:  torch.Size([10, 2, 64])\n",
            "Seems fine\n",
            "Testing AnswerVectorizer ...\n",
            "e:  torch.Size([10, 2, 64])\n",
            "Seems fine\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-SsMeO6WRcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a5eac5f-bd84-43b7-9c46-f69b8c10eb0d"
      },
      "source": [
        "dummy_v.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Upf0vSJqzvDl",
        "colab_type": "text"
      },
      "source": [
        "### Training: loss function (3 points)\n",
        "We want our vectorizers to put correct answers closer to question vectors and incorrect answers farther away from them. One way to express this is to use is Pairwise Hinge Loss _(aka Triplet Loss)_. \n",
        "\n",
        "$$ L = \\frac 1N \\underset {q, a^+, a^-} \\sum max(0, \\space \\delta - sim[V_q(q), V_a(a^+)] + sim[V_q(q), V_a(a^-)] )$$\n",
        "\n",
        ", where\n",
        "* sim[a, b] is some similarity function: dot product, cosine or negative distance\n",
        "* δ - loss hyperparameter, e.g. δ=1.0. If sim[a, b] is linear in b, all δ > 0 are equivalent.\n",
        "\n",
        "\n",
        "This reads as __Correct answers must be closer than the wrong ones by at least δ.__\n",
        "\n",
        "![img](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/margin.png)\n",
        "<center>_image: question vector is green, correct answers are blue, incorrect answers are red_</center>\n",
        "\n",
        "\n",
        "Note: in effect, we train a Deep Semantic Similarity Model [DSSM](https://www.microsoft.com/en-us/research/project/dssm/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlCQSkPxzvDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sim(a,b):\n",
        "  return torch.dot(a.view(-1),b.view(-1))\n",
        "\n",
        "def compute_loss(anchors, positives, negatives, delta=1):\n",
        "    \"\"\" \n",
        "    Compute the triplet loss:\n",
        "    \n",
        "    max(0, delta + sim(anchors, negatives) - sim(anchors, positives))\n",
        "    \n",
        "    where sim is a dot-product between vectorized inputs\n",
        "    \n",
        "    \"\"\"\n",
        "#     loss = torch.max( delta + sim(anchors, negatives) - sim(anchors, positives) ,0)\n",
        "    loss = delta + sim(anchors, negatives) - sim(anchors, positives)\n",
        "\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHdS0yjezvDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_recall(anchors, positives, negatives, delta=1):\n",
        "    \"\"\"\n",
        "    Compute the probability (ratio) at which sim(anchors, negatives) is greater than sim(anchors, positives)\n",
        "    \"\"\"\n",
        "    ratio = sim(anchors, negatives)/sim(anchors, positives) \n",
        "    return ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRHjzQztzvDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a3402f3d-f8d8-4c5a-f5ab-76484494407d"
      },
      "source": [
        "_dummy_anchors=question_vectorizer(dummy_batch[\"questions\"])\n",
        "_dummy_positives = answer_vectorizer(dummy_batch[\"correct_answers\"])\n",
        "_dummy_negatives = answer_vectorizer(dummy_batch[\"wrong_answers\"])\n",
        "print( compute_loss(_dummy_anchors, _dummy_positives, _dummy_negatives) )\n",
        "print( compute_recall(_dummy_anchors, _dummy_positives, _dummy_negatives) )"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0219, grad_fn=<SubBackward0>)\n",
            "tensor(1.0329, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAxDxnGrzvDr",
        "colab_type": "text"
      },
      "source": [
        "### Training loop (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "BdbIN9MFzvDr",
        "colab_type": "text"
      },
      "source": [
        "For a difference, we'll ask __you__ to implement training loop this time.\n",
        "\n",
        "Here's a sketch of one epoch:\n",
        "1. iterate over __`batches_per_epoch`__ batches from __`train_data`__ with __`iterate_minibatches`__\n",
        "    * Compute loss, backprop, optimize\n",
        "    * Compute and accumulate recall\n",
        "    \n",
        "2. iterate over __`batches_per_epoch`__ batches from __`val_data`__\n",
        "    * Compute and accumulate recall\n",
        "    \n",
        "3. print stuff :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaXlxhxVzvDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 100\n",
        "max_len = 100\n",
        "batch_size = 32\n",
        "batches_per_epoch = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YVh7puElwSs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "90f83d9a-c53b-4bb8-bc05-de37ec1280b4"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "question_vectorizer = QuestionVectorizer(out_size=max_len)\n",
        "answer_vectorizer = AnswerVectorizer(out_size=max_len)\n",
        "\n",
        "optimizer = torch.optim.Adam(chain(question_vectorizer.parameters(),\n",
        "                             answer_vectorizer.parameters()))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tet-fHLqzvDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "68643a75-2242-42af-ced6-affe7c5e1a02"
      },
      "source": [
        "for e in range(batches_per_epoch):\n",
        "  for data in iterate_minibatches(train, batch_size):\n",
        "    \n",
        "    _dummy_anchors=question_vectorizer(dummy_batch[\"questions\"])\n",
        "    _dummy_positives = answer_vectorizer(dummy_batch[\"correct_answers\"])\n",
        "    _dummy_negatives = answer_vectorizer(dummy_batch[\"wrong_answers\"])\n",
        "\n",
        "  #   print( compute_loss(_dummy_anchors, _dummy_positives, _dummy_negatives) )\n",
        "    \n",
        "\n",
        "    loss = compute_loss(_dummy_anchors, _dummy_positives, _dummy_negatives)\n",
        "\n",
        "    question_vectorizer.zero_grad()\n",
        "    answer_vectorizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print( compute_recall(_dummy_anchors, _dummy_positives, _dummy_negatives) ) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.7308, grad_fn=<DivBackward0>)\n",
            "tensor(-1.2440, grad_fn=<DivBackward0>)\n",
            "tensor(-1.2359, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rD3deRdzvDx",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Let's see how our model performs on actual question answering. You will score answer candidates with your model and select the most appropriate one.\n",
        "\n",
        "__Your goal__ is to obtain accuracy of at least above 50%. Beating 65% in this notebook yields bonus points :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziEOqbLgzvDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optional: prepare some functions here\n",
        "# <...>\n",
        "\n",
        "def select_best_answer(question, possible_answers):\n",
        "  \n",
        "    \"\"\"\n",
        "    Predicts which answer best fits the question\n",
        "    :param question: a single string containing a question\n",
        "    :param possible_answers: a list of strings containing possible answers\n",
        "    :returns: integer - the index of best answer in possible_answer\n",
        "    \"\"\"\n",
        "  \n",
        "    q=question_vectorizer(question)\n",
        "    nans=0\n",
        "    now = 1e9\n",
        "    \n",
        "    for i , a in enumirate(possible_answers):\n",
        "      ans=answer_vectorizer(a)\n",
        "      if(sim(q, ans)< now):\n",
        "        now = sim(q, ans)\n",
        "        nans=i\n",
        "        \n",
        "    return possible_answers[nans]\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVgvCo1ozvDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_answers = [\n",
        "    select_best_answer(question, possible_answers)\n",
        "    for i, (question, possible_answers) in tqdm(test[['question', 'options']].iterrows(), total=len(test))\n",
        "]\n",
        "\n",
        "accuracy = np.mean([\n",
        "    answer in correct_ix\n",
        "    for answer, correct_ix in zip(predicted_answers, test['correct_indices'].values)\n",
        "])\n",
        "print(\"Accuracy: %0.5f\" % accuracy)\n",
        "assert accuracy > 0.65, \"we need more accuracy!\"\n",
        "print(\"Great job!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEOheIsyzvDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_results(question, possible_answers, predicted_index, correct_indices):\n",
        "    print(\"Q:\", question, end='\\n\\n')\n",
        "    for i, answer in enumerate(possible_answers):\n",
        "        print(\"#%i: %s %s\" % (i, '[*]' if i == predicted_index else '[ ]', answer))\n",
        "    \n",
        "    print(\"\\nVerdict:\", \"CORRECT\" if predicted_index in correct_indices else \"INCORRECT\", \n",
        "          \"(ref: %s)\" % correct_indices, end='\\n' * 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYJS6w6kzvD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in [1, 100, 1000, 2000, 3000, 4000, 5000]:\n",
        "    draw_results(test.iloc[i].question, test.iloc[i].options,\n",
        "                 predicted_answers[i], test.iloc[i].correct_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R8hFOwPzvD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question = \"What is my name?\" # your question here!\n",
        "possible_answers = [\n",
        "    <...> \n",
        "    # ^- your options. \n",
        "]\n",
        "predicted answer = select_best_answer(question, possible_answers)\n",
        "\n",
        "draw_results(question, possible_answers,\n",
        "             predicted_answer, [0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "RAmhaEaFzvD2",
        "colab_type": "text"
      },
      "source": [
        "### Bonus tasks\n",
        "\n",
        "There are many ways to improve our question answering model. Here's a bunch of things you can do to increase your understanding and get bonus points.\n",
        "\n",
        "\n",
        "### 0. Fine-tuning (3+ pts)\n",
        "This time our dataset is fairly small. We can improve the training procedure by starting with a pre-trained model.\n",
        "* The simplest option is to use pre-trained embeddings. See previous weeks for that.\n",
        "* A harder (but better) alternative is to use a pre-trained sentence encoder. Consider [InferSent](https://github.com/facebookresearch/InferSent), Universal Sentence Encoder or ELMO.\n",
        "\n",
        "\n",
        "### 1.  Hard Negatives (3+ pts)\n",
        "\n",
        "Not all wrong answers are equally wrong. As the training progresses, _most negative examples $a^-$ will be to easy._ So easy in fact, that loss function and gradients on such negatives is exactly __0.0__. To improve training efficiency, one can __mine hard negative samples__.\n",
        "\n",
        "Given a list of answers,\n",
        "* __Hard negative__ is the wrong answer with highest similarity with question,\n",
        "\n",
        "$$a^-_{hard} = \\underset {a^-} {argmax} \\space sim[V_q(q), V_a(a^-)]$$\n",
        "\n",
        "* __Semi-hard negative__ is the one with highest similarity _among wrong answers that are farther than positive one. This option is more useful if some wrong answers may actually be mislabelled correct answers.\n",
        "\n",
        "* One can also __sample__ negatives proportionally to $$P(a^-_i) \\sim e ^ {sim[V_q(q), V_a(a^-_i)]}$$\n",
        "\n",
        "\n",
        "The task is to implement at least __hard negative__ sampling and apply it for model training.\n",
        "\n",
        "\n",
        "### 2. Bring Your Own Model (3+ pts)\n",
        "In addition to Universal Sentence Encoder, one can also train a new model.\n",
        "* You name it: convolutions, RNN, self-attention\n",
        "* Use pre-trained ELMO or FastText embeddings\n",
        "* Monitor overfitting and use dropout / word dropout to improve performance\n",
        "\n",
        "__Note:__ if you use ELMO please note that it requires tokenized text while USE can deal with raw strings. You can tokenize data manually or use tokenized=True when reading dataset.\n",
        "\n",
        "\n",
        "* hard negatives (strategies: hardest, hardest farter than current, randomized)\n",
        "* train model on the full dataset to see if it can mine answers to new questions over the entire wikipedia. Use approximate nearest neighbor search for fast lookup.\n",
        "\n",
        "\n",
        "### 3. Search engine (3+ pts)\n",
        "\n",
        "Our basic model only selects answers from 2-5 available sentences in paragraph. You can extend it to search over __the whole dataset__. All sentences in all other paragraphs are viable answers.\n",
        "\n",
        "The goal is to train such a model and use it to __quickly find top-10 answers from the whole set__.\n",
        "\n",
        "* You can ask such model a question of your own making - to see which answers it can find among the entire training dataset or even the entire wikipedia.\n",
        "* Searching for top-K neighbors is easier if you use specialized methods: [KD-Tree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) or [HNSW](https://github.com/nmslib/hnswlib). \n",
        "* This task is much easier to train if you use hard or semi-hard negatives. You can even find hard negatives for one question from correct answers to other questions in batch - do so in-graph for maximum efficiency. See [1.] for more details.\n"
      ]
    }
  ]
}